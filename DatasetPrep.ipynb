{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shaaagri/iat481-cv-proj/blob/main/DatasetPrep.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><h1> <b>Wildlife Detection CV Project - Dataset Preparation</b> </center> </h1>\n",
        "\n",
        "This notebook is based on Week 6 tutorial, we use it to check, enumerate, analyse our datasets in terms of correct labeling and balance, before merging them and doing the training with YOLO."
      ],
      "metadata": {
        "id": "7Le1bvcPKxgI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prerequisites:"
      ],
      "metadata": {
        "id": "8jbtjhzVKgid"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/MyDrive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcopSGdNKgSo",
        "outputId": "12d9b7e6-91f4-4a13-e749-b45ac45b9b24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/MyDrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The labels for this dataset are stored in YOLO format. i.e.\n",
        "\n",
        "    class_id bbox_x_center bbox_y_center bbox_width bbox_height"
      ],
      "metadata": {
        "id": "4GmdTvHOLWCS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our pest animals classes:"
      ],
      "metadata": {
        "id": "H-uKtcl2LiYD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_labels = {\n",
        "    0: \"bear\",\n",
        "    1: \"raccoon\",\n",
        "    2: \"rat\",\n",
        "    3: \"skunk\"\n",
        "}"
      ],
      "metadata": {
        "id": "dqGwCTe9LoIV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import the required libraries:"
      ],
      "metadata": {
        "id": "s5pmk-k3L7EN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random as random\n",
        "import os"
      ],
      "metadata": {
        "id": "BReXY07pL8I8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining paths to the collected datasets, both found and the ones we prepared ourselves:"
      ],
      "metadata": {
        "id": "cyOlfbRIMwTS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_paths = {\n",
        "  \"ours_bear\": \"/content/MyDrive/MyDrive/IAT481/481 CV Project/Datasets/Ours/Bear\",\n",
        "  \"ours_raccoon\": \"/content/MyDrive/MyDrive/IAT481/481 CV Project/Datasets/Ours/Raccoon\",\n",
        "  \"ours_rat\": \"/content/MyDrive/MyDrive/IAT481/481 CV Project/Datasets/Ours/Rat\",\n",
        "  \"ours_skunk\": \"/content/MyDrive/MyDrive/IAT481/481 CV Project/Datasets/Ours/Skunk\",\n",
        "  \"found_bear\": \"/content/MyDrive/MyDrive/IAT481/481 CV Project/Datasets/Found/Bear/Bear.v1i.yolov8\",\n",
        "  \"found_raccoon\": \"/content/MyDrive/MyDrive/IAT481/481 CV Project/Datasets/Found/Raccoon/Raccoon.v38-416x416-resize.yolov8\",\n",
        "  \"found_rat\": \"/content/MyDrive/MyDrive/IAT481/481 CV Project/Datasets/Found/Rat/cc-object-detection.v6i.yolov8\",\n",
        "  \"found_skunk\": \"/content/MyDrive/MyDrive/IAT481/481 CV Project/Datasets/Found/Skunk/Skunk.v1i.yolov8\",\n",
        "}"
      ],
      "metadata": {
        "id": "G2OH8Mc_MsNB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Collect all image and label files we've got and save into one data structure:"
      ],
      "metadata": {
        "id": "V-ardTmZHkcm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def enumerate_dataset(dataset_id, dataset_path):\n",
        "  image_paths, label_paths = {}, {}\n",
        "\n",
        "  try:\n",
        "    # sorting the directories listings so that images' and labels' positions in lists match\n",
        "    image_paths['train'] = [os.path.join(dataset_path, \"train\", \"images\", image_filename) for image_filename in sorted(os.listdir(os.path.join(dataset_path, \"train\", \"images\")))]\n",
        "    image_paths['valid'] = [os.path.join(dataset_path, \"valid\", \"images\", image_filename) for image_filename in sorted(os.listdir(os.path.join(dataset_path, \"valid\", \"images\")))]\n",
        "    image_paths['test'] = [os.path.join(dataset_path, \"test\", \"images\", image_filename) for image_filename in sorted(os.listdir(os.path.join(dataset_path, \"test\", \"images\")))]\n",
        "  except FileNotFoundError:\n",
        "    pass\n",
        "\n",
        "  try:\n",
        "    label_paths['train'] = [os.path.join(dataset_path, \"train\", \"labels\", image_filename) for image_filename in sorted(os.listdir(os.path.join(dataset_path, \"train\", \"labels\")))]\n",
        "    label_paths['valid'] = [os.path.join(dataset_path, \"valid\", \"labels\", image_filename) for image_filename in sorted(os.listdir(os.path.join(dataset_path, \"valid\", \"labels\")))]\n",
        "    label_paths['test'] = [os.path.join(dataset_path, \"test\", \"labels\", image_filename) for image_filename in sorted(os.listdir(os.path.join(dataset_path, \"test\", \"labels\")))]\n",
        "  except FileNotFoundError:\n",
        "    pass\n",
        "\n",
        "  return image_paths, label_paths\n",
        "\n",
        "def enumerate_dataset_multi(dataset_paths):\n",
        "  return {dataset_id: enumerate_dataset(dataset_id, dataset_paths[dataset_id]) for dataset_id in dataset_paths}"
      ],
      "metadata": {
        "id": "MJY5CW1ZL_nw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datasets = enumerate_dataset_multi(dataset_paths)"
      ],
      "metadata": {
        "id": "yJaxxSXlUjTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to easy count the contents of a dataset by class across 'train', 'valid', and 'test' parts\n",
        "def count_samples(dataset):\n",
        "  sample_counts = {}\n",
        "\n",
        "  for k, v in dataset.items():\n",
        "    sample_counts[k] = [len(v['images']['train']), len(v['images']['valid']), len(v['images']['test'])]\n",
        "\n",
        "  return sample_counts\n",
        "\n",
        "# create a writeable place in the dataset dictionary for a class\n",
        "def initialize_class_in_dataset(cl, dataset):\n",
        "  dataset[cl] = {}\n",
        "  dataset[cl]['images'] = {}\n",
        "  dataset[cl]['images']['train'] = []\n",
        "  dataset[cl]['images']['valid'] = []\n",
        "  dataset[cl]['images']['test'] = []\n",
        "  dataset[cl]['labels'] = {}\n",
        "  dataset[cl]['labels']['train'] = []\n",
        "  dataset[cl]['labels']['valid'] = []\n",
        "  dataset[cl]['labels']['test'] = []"
      ],
      "metadata": {
        "id": "H0R4-tEAF7sc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Merging all the data we have into N strict classes (defined earlier). Doing our best with the totality of data we've got to make sure the classes are balanced."
      ],
      "metadata": {
        "id": "lBpRnSb9Hobs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_ids = dataset_paths.keys()\n",
        "\n",
        "merged_dataset = {}\n",
        "\n",
        "for cl in class_labels.values():\n",
        "  for id in dataset_ids:\n",
        "    if cl in id:\n",
        "\n",
        "      if cl not in merged_dataset:\n",
        "        initialize_class_in_dataset(cl, merged_dataset)\n",
        "\n",
        "      # ignoring KeyError, as some datasets may have only a 'train' folder, but not the others\n",
        "      try:\n",
        "        merged_dataset[cl]['images']['train'] += datasets[id][0]['train']\n",
        "        merged_dataset[cl]['labels']['train'] += datasets[id][1]['train']\n",
        "      except KeyError:\n",
        "        pass\n",
        "\n",
        "      try:\n",
        "        merged_dataset[cl]['images']['valid'] += datasets[id][0]['valid']\n",
        "        merged_dataset[cl]['labels']['valid'] += datasets[id][1]['valid']\n",
        "      except KeyError:\n",
        "        pass\n",
        "\n",
        "      try:\n",
        "        merged_dataset[cl]['images']['test'] += datasets[id][0]['test']\n",
        "        merged_dataset[cl]['labels']['test'] += datasets[id][1]['test']\n",
        "      except KeyError:\n",
        "        pass\n",
        "\n",
        "# undersampling all classes to match the lowest amount of train samples available - this will result in the balance we look for\n",
        "train_samples_count = [v[0] for k, v in count_samples(merged_dataset).items()]\n",
        "min_train_samples_count = np.array(train_samples_count).min()\n",
        "\n",
        "balanced_dataset = {}\n",
        "\n",
        "for cl, v in merged_dataset.items():\n",
        "  if cl not in balanced_dataset:\n",
        "    initialize_class_in_dataset(cl, balanced_dataset)\n",
        "\n",
        "  # Fill the training set, capping at min_train_samples_count\n",
        "  balanced_dataset[cl]['images']['train'] = v['images']['train'][:min_train_samples_count]\n",
        "  balanced_dataset[cl]['labels']['train'] = v['labels']['train'][:min_train_samples_count]\n",
        "\n",
        "  # Take out the samples from the original merged dataset\n",
        "  merged_dataset[cl]['images']['train'] = v['images']['train'][min_train_samples_count:]\n",
        "  merged_dataset[cl]['labels']['train'] = v['labels']['train'][min_train_samples_count:]\n",
        "\n",
        "\n",
        "sample_pool = {}\n",
        "\n",
        "# try to reach 80/10/10 ratios for our train-valid-test split\n",
        "valid_test_target_sample_count = round(min_train_samples_count * 0.125)\n",
        "\n",
        "#print(valid_test_target_sample_count)\n",
        "\n",
        "for cl, data in merged_dataset.items():\n",
        "  # dumping all leftover samples into one list per class\n",
        "  sample_pool = {}\n",
        "  sample_pool['images'] = []\n",
        "  sample_pool['labels'] = []\n",
        "\n",
        "  sample_pool['images'] += [i for i in data['images']['train']]\n",
        "  sample_pool['images'] += [i for i in data['images']['valid']]\n",
        "  sample_pool['images'] += [i for i in data['images']['test']]\n",
        "  sample_pool['labels'] += [l for l in data['labels']['train']]\n",
        "  sample_pool['labels'] += [l for l in data['labels']['valid']]\n",
        "  sample_pool['labels'] += [l for l in data['labels']['test']]\n",
        "\n",
        "  n = valid_test_target_sample_count\n",
        "\n",
        "  # finally, populating our validation and testing set\n",
        "  balanced_dataset[cl]['images']['valid'] = sample_pool['images'][:n]\n",
        "  balanced_dataset[cl]['labels']['valid'] = sample_pool['labels'][:n]\n",
        "  balanced_dataset[cl]['images']['test'] = sample_pool['images'][n:n+n]\n",
        "  balanced_dataset[cl]['labels']['test'] = sample_pool['labels'][n:n+n]"
      ],
      "metadata": {
        "id": "kPJAuCzo7Idi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking the train-valid-test distribution of samples in our final, merged and balanced dataset:"
      ],
      "metadata": {
        "id": "QyI5mrrBgKgb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(count_samples(balanced_dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tUWVuD6JgZKM",
        "outputId": "3c1c489f-f99c-4faa-9624-11a64c7b1942"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'bear': [162, 20, 20], 'raccoon': [162, 20, 20], 'rat': [162, 20, 20], 'skunk': [162, 20, 20]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Awesome! Now, just let's make sure the dataset does not require any further cleaning, meaning there are no images with missing labels, or orphan labels that do not belong to any image:"
      ],
      "metadata": {
        "id": "nnmkXwbSg1bT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dataset_cleanness_check(dataset):\n",
        "  image_files = set()\n",
        "  label_files = set()\n",
        "\n",
        "  for cl, data in dataset.items():\n",
        "    for subset_type in ['train', 'valid', 'test']:\n",
        "      image_files.update({i.split(\"/\")[-1].split(\".\")[0] for i in data['images'][subset_type]})\n",
        "      label_files.update({l.split(\"/\")[-1].split(\".\")[0] for l in data['labels'][subset_type]})\n",
        "\n",
        "  print(f\"Extra images (without corresponding labels): {image_files - label_files}\")\n",
        "  print(f\"Extra labels (without corresponding images): {label_files - image_files}\")"
      ],
      "metadata": {
        "id": "OMK4Hm26hNcO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_cleanness_check(balanced_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpXCmI_0hfHS",
        "outputId": "56d762de-6370-4df4-c9c5-6bcb60f7dc80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extra images (without corresponding labels): set()\n",
            "Extra labels (without corresponding images): set()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are none extra images or labels! Great. Now we are ready to save the merged dataset on disk (till this point we have been just manipulating file references) and proceed to training."
      ],
      "metadata": {
        "id": "se-PmSsilSpO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import os\n",
        "#run it once for making directories\n",
        "\n",
        "final_dataset_root = '/content/MyDrive/MyDrive/IAT481/481 CV Project/Datasets/Final'\n",
        "\n",
        "os.makedirs('/content/MyDrive/MyDrive/IAT481/481 CV Project/Datasets/Final', exist_ok=True)\n",
        "os.makedirs('/content/MyDrive/MyDrive/IAT481/481 CV Project/Datasets/Final/train', exist_ok=True)\n",
        "os.makedirs('/content/MyDrive/MyDrive/IAT481/481 CV Project/Datasets/Final/train/images', exist_ok=True)\n",
        "os.makedirs('/content/MyDrive/MyDrive/IAT481/481 CV Project/Datasets/Final/train/labels', exist_ok=True)\n",
        "os.makedirs('/content/MyDrive/MyDrive/IAT481/481 CV Project/Datasets/Final/valid', exist_ok=True)\n",
        "os.makedirs('/content/MyDrive/MyDrive/IAT481/481 CV Project/Datasets/Final/valid/images', exist_ok=True)\n",
        "os.makedirs('/content/MyDrive/MyDrive/IAT481/481 CV Project/Datasets/Final/valid/labels', exist_ok=True)\n",
        "os.makedirs('/content/MyDrive/MyDrive/IAT481/481 CV Project/Datasets/Final/test', exist_ok=True)\n",
        "os.makedirs('/content/MyDrive/MyDrive/IAT481/481 CV Project/Datasets/Final/test/images', exist_ok=True)\n",
        "os.makedirs('/content/MyDrive/MyDrive/IAT481/481 CV Project/Datasets/Final/test/labels', exist_ok=True)"
      ],
      "metadata": {
        "id": "cq2n4N2vl2Sf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "\n",
        "class_labels_reversed = {class_labels[l]: l for l in class_labels.keys()}\n",
        "\n",
        "def save_merged_dataset():\n",
        "  for cl, data in balanced_dataset.items():\n",
        "    final_dataset_root\n",
        "\n",
        "    for subset_type in ['train', 'valid', 'test']:\n",
        "      for file_category in ['images', 'labels']:\n",
        "        for file in data[file_category][subset_type]:\n",
        "          if file_category == 'images':\n",
        "            if file.endswith(\".jpg\"):\n",
        "              image = Image.open(file)\n",
        "              image = image.convert(\"RGB\")\n",
        "\n",
        "              new_filename = os.path.splitext(os.path.basename(file))[0] + \".jpg\"\n",
        "              print(os.path.join(final_dataset_root, subset_type, \"images\", new_filename))\n",
        "\n",
        "              image.save(os.path.join(final_dataset_root, subset_type, \"images\", new_filename))\n",
        "\n",
        "          if file_category == 'labels':\n",
        "              with open(file, \"r\") as src:\n",
        "                lines = src.readlines()\n",
        "                lines_new = []\n",
        "\n",
        "                # forcing the first value in the line to be the correct class id\n",
        "                # (YOLO object detection labeling format)\n",
        "                for line in lines:\n",
        "                  line_s = line.split(' ')\n",
        "                  line_s[0] = str(class_labels_reversed[cl])\n",
        "                  lines_new.append(' '.join(line_s))\n",
        "\n",
        "                dst_file = os.path.join(final_dataset_root, subset_type, \"labels\", os.path.basename(file))\n",
        "\n",
        "                with open(dst_file, \"w\") as dst:\n",
        "                  dst.writelines(lines_new)"
      ],
      "metadata": {
        "id": "DHF-EVAxn9gC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_merged_dataset()"
      ],
      "metadata": {
        "id": "hPBZ3lOYxNqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The consolidated dataset should have been appeared in Drive when the code finished its job."
      ],
      "metadata": {
        "id": "2XsqYGYI_Pke"
      }
    }
  ]
}